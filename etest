#!/usr/bin/env bash

## Check if we're root and re-execute if we're not through sudo ##
if [[ $(id -u) != "0" ]]; then
    sudo -E "$0" "$@"
    exit $?
fi

: ${BASHUTILS:=$(dirname $0)}
source ${BASHUTILS}/bashutils.sh || { echo "Unable to source bashutils." ; exit 1 ; }
export BASHUTILS

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

# If no test directory was provided use PWD.
$(declare_args -g ?TEST_DIR)
: ${TEST_DIR:=$(readlink -f .)}
ETEST_TOPDIR=${TEST_DIR}

# OPTIONS:
# -f=FILTER: Optional filter of tests to run (defaults to FILTER which defaults to "").
# -r=NUM:    Number of times to repeat matching tests (defaults to REPEAT which defaults to 0).
# -v=(0|1)   Optional value for VERBOSE setting. If set to 0 you'll get terse output from etest showing
#            only the test name and result. If set to 1 you'll get more verbose output from etest and each test.
# -b=(0|1)   Break on failure (defaults to 0)
# -c=(0|1)   Clean only then exit.
FILTER=$(opt_get f ${FILTER:=""})
EXCLUDE=$(opt_get x ${EXCLUDE:=""})
REPEAT=$(opt_get r ${REPEAT:=1})
(( ${REPEAT} < 1 )) && REPEAT=1
VERBOSE=$(opt_get v 0)
[[ ${EDEBUG:-0} != "0" ]] && VERBOSE=1 || true
BREAK=$(opt_get b ${BREAK:=0})
edebug "$(lval TEST_DIR FILTER REPEAT VERBOSE BREAK)"

# Global cgroup name for all unit tests
export ETEST_CGROUP_BASE="etest/$(basename ${TEST_DIR})"
export ETEST_CGROUP="${ETEST_CGROUP_BASE}/$$"

ETEST_BREAK_EXIT_CODE=25

# Setup logfile
exec {ETEST_STDERR_FD}<&2

ETEST_LOG=${TEST_DIR}/etest.log
elogfile -r=1 -t=${VERBOSE} ${ETEST_LOG}

# Setup eTEST_OUTions for the "etest" outpu tand actual "test" output
if [[ ${VERBOSE} -eq 0 ]]; then
    ETEST_OUT="/proc/self/fd/${ETEST_STDERR_FD}"
    TEST_OUT="/dev/null"
else
    ETEST_OUT="/dev/null"
    TEST_OUT="/dev/stderr"
fi

#-------------------------------------------------------------------------------------------------------------------------
# TEST UTILITY FUNCTIONS
#-------------------------------------------------------------------------------------------------------------------------

etestmsg()
{
    EMSG_COLOR="all" emsg "magenta" "##" "INFO" "$@"
}

# Returns success if there are no stale processes remaining in the cgroup for
# this test and failure if there are any.
#
no_process_leaks_remain()
{
    $(tryrc -r=exists_rc cgroup_exists ${ETEST_CGROUP})

    # If the cgroup no longer exists, we're in good shape because you can't
    # destroy a cgroup until all its processes are dead.
    if [[ ${exists_rc} -ne 0 ]] ; then
        return 0
    fi

    # As long as it existed just now, we can assume cgroup_pids will exist,
    # because nothing else will destroy the cgroup except for us.
    local remaining_pids=$(cgroup_pids ${ETEST_CGROUP})
    edebug "$(lval remaining_pids exists_rc ETEST_CGROUP)"
    [[ -z ${remaining_pids} ]]
}

assert_no_process_leaks()
{
    # Wait for up to 5 seconds for leaked processes to die off.  If anything
    # lasts beyond that, we'll call it a test failure.
    $(tryrc eretry -d=.1s -T=5s -w=1 no_process_leaks_remain)

    if [[ ${rc} -ne 0 ]] ; then
        local leaked_processes=$(cgroup_ps ${ETEST_CGROUP})

        cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP}

        die "Leaked processes:\n${leaked_processes}"
    fi
}

global_setup()
{
    edebug "Starting global_setup"

    # Create a specific directory to run this test in. That way the test can create whatever directories and files it
    # needs and assuming the test succeeds we'll auto remove the directory after the test completes.
    TEST_DIR_OUTPUT="${TEST_DIR}/output"
    efreshdir ${TEST_DIR_OUTPUT}

    # And a cgroup that will contain all output
    cgroup_create ${ETEST_CGROUP_BASE}
    cgroup_move ${ETEST_CGROUP_BASE} $$

    edebug "Finished global_setup"
    return 0
}

global_teardown()
{
    edebug "Starting global_teardown: PID=$$ BASHPID=${BASHPID} PPID=${PPID}"

    eunmount_recursive ${TEST_DIR_OUTPUT} &>$(edebug_out)
    
    edebug "Finished global_teardown"
    return 0
}

run_single_test()
{
    local testfile=$1
    local testfunc=$2
    local testfilename=$(basename ${testfile})
    local rc=0

    source ${testfile}
    declare -f ${testfunc} &>/dev/null || return 0

    ebanner "${testfunc}" REPEAT=REPEAT_STRING
 
    einfos ${testfunc} &>${ETEST_OUT}
   
    # We want to make sure that any traps from the tests
    # execute _before_ we run teardown, and also we don't
    # want the teardown to run inside the test-specific
    # cgroup.  This subshell solves both issues.
    try
    {
        cgroup_create ${ETEST_CGROUP}
        cgroup_move ${ETEST_CGROUP} ${BASHPID}

        # Unit test provided setup
        if declare -f setup &>/dev/null ; then
            etestmsg "Calling test_setup"
            setup
        fi
        
        cd ${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}

        etestmsg "Calling test"
        ${testfunc}
    }
    catch
    {
        rc=$?
    }

    if [[ ${rc} -eq 0 ]]; then
        einfo "$(ecolor green)${testfunc} PASSED."
    else
        eerror "${testfunc} FAILED."
    fi

    # Unit test provided teardown
    if declare -f teardown &>/dev/null ; then
        etestmsg "Calling test_teardown"
        $(tryrc teardown)
    fi

    return ${rc}
}

run_all_tests_in_file()
{
    local testfile=$1
    local testfilename=$(basename ${testfile})

    # Skip files that don't contain any tests
    grep -q "^ETEST_" ${testfile} >/dev/null || return 0

    # Skip files whose name matches the exclude filter
    [[ -n ${EXCLUDE} && ${testfile} =~ ${EXCLUDE} ]] && return 0

    # Ensure proper shebang is in the unit test or else bashlint won't be able to validate it
    # and it may not be a proper bash unit test
    grep -q '^#!/.*bash' ${testfile} || { eerror "No bash shebang in ${testfile}"; throw 1; }

    # Get all function names that begin with ETEST_ (and optionally match $2)
    if [[ ${testfile} =~ ${FILTER} ]]; then
        ETEST_FUNCTIONS=( $(source ${testfile}; declare -F | awk '$3 ~ "^ETEST" {print $3}' ) )
    else
        ETEST_FUNCTIONS=( $(source ${testfile}; declare -F | awk '$3 ~ "^ETEST" && $3 ~ "'${FILTER}'" {print $3}') )
    fi
    edebug $(lval testfile ETEST_FUNCTIONS)

    [[ ${#ETEST_FUNCTIONS[@]} -gt 0 ]] || return 0

    # Exclude those functions that match the exclude filter
    if [[ -n ${EXCLUDE} ]] ; then
        for index in "${!ETEST_FUNCTIONS[@]}" ; do
            if [[ ${ETEST_FUNCTIONS[$index]} =~ ${EXCLUDE} ]] ; then
                unset ETEST_FUNCTIONS[$index]
            fi
        done
    fi

    einfo "Starting tests in ${testfile} ${REPEAT_STRING}" &>${ETEST_OUT}

    for testfunc in file_setup ${ETEST_FUNCTIONS[@]} file_teardown; do
     
        # Unit test infrastructure setup
        efreshdir ${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}
       
        try
        {
            run_single_test ${testfile} ${testfunc} |& tee -a ${ETEST_LOG} &>${TEST_OUT}
            eend &>${ETEST_OUT}
        }
        catch
        {
            eend $? &>${ETEST_OUT}
            [[ ${BREAK} -eq 0 ]] || die "${testfunc} failed and BREAK=1" &>${ETEST_OUT}
            FAILURES[${testfilename}]+="${testfunc//ETEST_} "
        }

        # Unit test infrastructure teardown
        assert_no_process_leaks
        eunmount_recursive ${TEST_DIR_OUTPUT}/${testfilename}/${testfunc} &>$(edebug_out)
        rm -rf ${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}
    done
}

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

declare -A FAILURES
global_setup
trap_add global_teardown

# If clean only is requested exit immediately. The "clean" is done via global_setup and global_teardown.
opt_true c && exit 0

#-------------------------------------------------------------------------------------------------------------------------
# SANITY TEST OF DIE FUNCTIONALITY
#-------------------------------------------------------------------------------------------------------------------------

if [[ -x ${ETEST_TOPDIR}/unittest/die && -z ${FILTER} && ! ${EXCLUDE} =~ "die" ]] ; then

    # Manually test die() functionality outside of try/catch to ensure basic trap functionality works properly.
    DIE_TEST_MSG="Verifying die() trap functionality"
    edebug_enabled && ebanner "${DIE_TEST_MSG}" || einfo "${DIE_TEST_MSG}"
    {
        export DIE_FILE="etest_die.txt"
        ${ETEST_TOPDIR}/unittest/die

        # Assert proper order of events happened
        einfo "Output file:"
        cat ${DIE_FILE}
        assert_eq 2 $(wc -l ${DIE_FILE})
        first=$(head -1 ${DIE_FILE})
        last=$(tail -1 ${DIE_FILE})

        assert_eq "DIE" "${first}" "die() not called first"
        assert_eq "TRAP" "${last}" "trap not called second"

    } &>$(edebug_out)

    rm -f ${DIE_FILE}

    # Clean up so that die test doesn't affect future ones
    unset DIE_FILE first last
    unset -f die_handler

    eend
fi

#-------------------------------------------------------------------------------------------------------------------------
# MAIN
#-------------------------------------------------------------------------------------------------------------------------

# Now go through and test all unit tests
for (( ITERATION=1; ITERATION<=${REPEAT}; ITERATION++ )); do
    [[ ${REPEAT} -gt 1 ]] && REPEAT_STRING="(${ITERATION}/${REPEAT})" || REPEAT_STRING=""
    for filename in $(find ${TEST_DIR} -type f -name "*.sh" | sort || true); do
        run_all_tests_in_file ${filename}
    done
done

if array_not_empty FAILURES; then
    eerror "$(lval FAILURES)"
    exit ${#FAILURES[@]}
fi

exit 0
