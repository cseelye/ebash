#!/usr/bin/env bash

## Check if we're root and re-execute if we're not through sudo ##
if [[ $(id -u) != "0" ]]; then
    sudo -E "$0" "$@"
    exit $?
fi

: ${BASHUTILS:=$(dirname $0)}
source ${BASHUTILS}/bashutils.sh || { echo "Unable to source bashutils." ; exit 1 ; }

# Parse arguments. If no test directory was provided use PWD.
$(declare_args -g ?TEST_DIR)
: ${TEST_DIR:=$(readlink -f .)}
ETEST_TOPDIR=${TEST_DIR}

# OPTIONS:
# -f=FILTER: Optional filter of tests to run (defaults to FILTER which defaults to "").
# -r=NUM:    Number of times to repeat matching tests (defaults to REPEAT which defaults to 0).
# -v=(0|1)   Optional value for VERBOSE setting. If set to 0 you'll get terse output from etest showing
#            only the test name and result. If set to 1 you'll get more verbose output from etest and each test.
# -b=(0|1)   Break on failure (defaults to 0)
FILTER=$(opt_get f ${FILTER:=""})
REPEAT=$(opt_get r ${REPEAT:=0})
VERBOSE=$(opt_get v 0)
[[ ${EDEBUG:-0} != "0" ]] && VERBOSE=1 || true
BREAK=$(opt_get b ${BREAK:=0})
edebug "$(lval TEST_DIR FILTER REPEAT VERBOSE BREAK)"

# Global cgroup name for all unit tests
export ETEST_CGROUP_BASE="etest/$(basename ${TEST_DIR})"
export ETEST_CGROUP="${ETEST_CGROUP_BASE}/$$"

#-------------------------------------------------------------------------------------------------------------------------
# TEST UTILITY FUNCTIONS
#-------------------------------------------------------------------------------------------------------------------------

etestmsg()
{
    echo -e "$(EMSG_COLOR="all" emsg 'magenta' '##' 'INFO' "$@")" >&2
}

# Returns success if there are no stale processes remaining in the cgroup for
# this test and failure if there are any.
#
no_process_leaks_remain()
{
    $(tryrc -r=cgroup_exists cgroup_exists ${ETEST_CGROUP})

    # If the cgroup no longer exists, we're in good shape because you can't
    # destroy a cgroup until all its processes are dead.
    if [[ ${cgroup_exists} -ne 0 ]] ; then
        return 0;
    fi

    # As long as it existed just now, we can assume cgroup_pids will exist,
    # because nothing else will destroy the cgroup except for us.
    local remaining_pids=$(cgroup_pids ${ETEST_CGROUP})
    edebug "$(lval remaining_pids cgroup_exists ETEST_CGROUP)"
    [[ -z ${remaining_pids} ]]
}

assert_no_process_leaks()
{
    cgroup_move "${ETEST_CGROUP_BASE}" $$ ${BASHPID}

    # Wait for up to 5 seconds for leaked processes to die off.  If anything
    # lasts beyond that, we'll call it a test failure.
    $(tryrc eretry -T=5s no_process_leaks_remain)

    if [[ ${rc} -eq 0 ]] ; then
        cgroup_destroy -r ${ETEST_CGROUP}
    
    else
        einfo "Killing stale test processes"
        local leaked_processes=$(cgroup_ps ${ETEST_CGROUP})

        cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP}
        cgroup_destroy -r ${ETEST_CGROUP}

        die "Leaked processes:\n${leaked_processes}"

    fi
}

global_setup()
{
    edebug "Starting global_setup"

    # Create a specific directory to run this test in. That way the test can create whatever directories and files it
    # needs and assuming the test succeeds we'll auto remove the directory after the test completes.
    TEST_DIR_OUTPUT="${TEST_DIR}/output"
    mkdir -p ${TEST_DIR_OUTPUT}

    if [[ -n "$(cgroup_pids ${ETEST_CGROUP_BASE})" ]]; then
        ewarn "Killing stale $(lval CGROUP=ETEST_CGROUP_BASE) processes=($(cgroup_pids ${ETEST_CGROUP_BASE}))"
        cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP_BASE}
    fi

    # And a cgroup that will contain all output
    cgroup_create ${ETEST_CGROUP}
    cgroup_move ${ETEST_CGROUP} $$

    edebug "Finished global_setup"
    return 0
}

global_teardown()
{
    edebug "Starting global_teardown: PID=$$ BASHPID=${BASHPID} PPID=${PPID}"

    assert_no_process_leaks
   
    argcheck TEST_DIR_OUTPUT
    if [[ ${ETEST_CLEAN:-1} -ne 0 ]] ; then
        eunmount_recursive ${TEST_DIR_OUTPUT} &>$(edebug_out)
        rm -rf ${TEST_DIR_OUTPUT}
    fi

    edebug "Finished global_teardown"
    return 0
}

call_file_setup()
{
    edebug "Starting call_file_setup"

    # Unit test infrastructure setup
    argcheck TEST_DIR_OUTPUT
    mkdir -p ${TEST_DIR_OUTPUT}/${testfile}

    # Unit test provided setup
    if declare -f file_setup &>/dev/null ; then
        einfos "FILE Setup"
        file_setup &> ${REDIRECT}
        eend
    fi

    edebug "Finished call_file_setup"
    return 0
}

call_file_teardown()
{
    edebug "Starting call_file_teardown"

    # Unit test provided teardown
    if declare -f file_teardown &>/dev/null ; then
        einfos "FILE Teardown"
        $(tryrc file_teardown &> ${REDIRECT})
        eend
    fi

    # Unit test infrastructure teardown
    if [[ ${ETEST_CLEAN:-1} -ne 0 ]] ; then
        eunmount_recursive ${TEST_DIR_OUTPUT}/${testfile} &>$(edebug_out)
        rm -rf ${TEST_DIR_OUTPUT}/${testfile}
    fi

    edebug "Finished call_file_teardown"
    return 0
}

call_test_setup()
{
    # Unit test infrastructure setup
    argcheck TEST_DIR_OUTPUT
    mkdir -p ${TEST_DIR_OUTPUT}/${testfile}/${testfunc}

    # Unit test provided setup
    if declare -f setup &>/dev/null ; then
        etestmsg "Calling test_setup"
        setup &> ${REDIRECT}
        eend
    fi

    cd ${TEST_DIR_OUTPUT}/${testfile}/${testfunc}
    
    return 0
}

call_test_teardown()
{
    edebug "Starting test_teardown"

    # Unit test provided teardown
    if declare -f teardown &>/dev/null ; then
        etestmsg "Calling test_teardown"
        $(tryrc teardown) &> ${REDIRECT}
        eend
    fi

    assert_no_process_leaks

    # Unit test infrastructure teardown
    argcheck testfunc
    if [[ ${ETEST_CLEAN:-1} -ne 0 ]] ; then
        eunmount_recursive ${TEST_DIR_OUTPUT}/${testfunc} &>$(edebug_out)
        rm -rf ${TEST_DIR_OUTPUT}/${testfunc}
    fi

    edebug "Finished test_teardown"
    return 0
}

#-------------------------------------------------------------------------------------------------------------------------
# SANITY TEST OF DIE FUNCTIONALITY
#-------------------------------------------------------------------------------------------------------------------------

# Manually test die() functionality outside of try/catch to ensure basic trap functionality works properly.
DIE_TEST_MSG="Verifying die() trap functionality"
edebug_enabled && ebanner "${DIE_TEST_MSG}" || einfo "${DIE_TEST_MSG}"
{
    DIE_FILE="etest_die.txt"
    rm -f ${DIE_FILE}

    die_handler()
    {
        edebug "DIE_HANDLER called"
        echo "DIE" >> ${DIE_FILE}
    }

    trap_add "einfo TRAP; echo TRAP >> ${DIE_FILE}"
    die -c=grey19 "Fake death..."

    # Assert proper order of events happened
    einfo "Ensuring TRAP was called before DIE"
    cat ${DIE_FILE}
    first=$(head -1 ${DIE_FILE})
    last=$(tail -1 ${DIE_FILE})

    [[ ${first} == "TRAP" ]] || { eerror "TRAP NOT CALLED FIRST"; exit 1; }
    [[ ${last}  == "DIE"  ]] || { eerror "DIE NOT CALLED LAST";   exit 1; }

} &>$(edebug_out)

rm -f ${DIE_FILE}

# Remove die_handler so we revert to fatal error handling
unset -f die_handler

eend

#-------------------------------------------------------------------------------------------------------------------------
# MAIN
#-------------------------------------------------------------------------------------------------------------------------

trap - ${DIE_SIGNALS[@]} ERR EXIT
die_on_abort
die_on_error

FAILED_FILES=()
global_setup
trap_add global_teardown

# Now go through and test all unit tests
for testfile in $(find ${TEST_DIR} -type f -name "*.sh" | sort || true); do

    [[ ${VERBOSE} -eq 1 ]] && REDIRECT="/dev/stderr" || REDIRECT="/dev/null"

    # Each test file gets its own subshell
    try
    {
        FAILED_TESTS=()

        # Skip files that don't contain any tests
        grep -q "^ETEST_" ${testfile} >/dev/null || continue

        source ${testfile}

        # Ensure proper shebang is in the unit test or else bashlint won't be able to validate it
        # and it may not be a proper bash unit test
        grep -q '^#!/.*bash' ${testfile} || { eerror "No bash shebang in ${testfile}"; throw 1; }

        # Get all function names that begin with ETEST_ (and optionally match $2)
        [[ ${testfile} =~ ${FILTER} ]] && ETEST_FUNCTIONS=( $(declare -F | awk '$3 ~ "^ETEST" {print $3}' ) ) \
                                       || ETEST_FUNCTIONS=( $(declare -F | awk '$3 ~ "^ETEST" && $3 ~ "'${FILTER}'" {print $3}') )

        [[ ${#ETEST_FUNCTIONS[@]} -gt 0 ]] || continue

        # Repeat the selected tests the requested number of times
        for (( ITERATION=0; ITERATION<=${REPEAT:-0}; ITERATION++ )); do
          
            [[ ${REPEAT} -gt 0 ]] && REPEAT_STRING="(${ITERATION}/${REPEAT})" || REPEAT_STRING=""

            [[ ${VERBOSE} -eq 1 ]] || einfo "Starting tests in ${testfile} ${REPEAT_STRING}"
            call_file_setup

            for testfunc in "${ETEST_FUNCTIONS[@]}" ; do

                # If debugging is not enabled for testfunc just show a brief message with test name
                testfunc_rc=0
                
                [[ ${VERBOSE} -eq 0 ]] && einfos ${testfunc} || ebanner ${testfunc} REPEAT=REPEAT_STRING

                {
                    try
                    {
                        call_test_setup
                        etestmsg "Calling test"
                        ${testfunc}
                    }
                    catch
                    {
                        testfunc_rc=$?
                        eend ${testfunc_rc}
                        FAILED_TESTS+=( ${testfunc/ETEST_/} )

                        [[ ${BREAK} -eq 0 ]] || die "${testfunc} failed and BREAK=1"
                    }
                    
                    call_test_teardown

                } &> ${REDIRECT}

                eend ${testfunc_rc}
               
            done

            call_file_teardown
        
        done

        array_empty FAILED_TESTS || { eerror_stacktrace "$(lval FAILED_TESTS)"; echo; throw 1; }
    } 
    catch
    {
        FAILED_FILES+=( $(basename ${testfile}) ) || true
    }

done

array_empty FAILED_FILES || eerror "$(lval FAILED_FILES)"

exit ${#FAILED_FILES[@]}
