#!/usr/bin/env bash

## Check if we're root and re-execute if we're not through sudo ##
if [[ $(id -u) != "0" ]]; then
    exec sudo -E "$0" "$@"
fi

: ${EMSG_PREFIX:=time}

: ${BASHUTILS:=$(dirname $0)}
source ${BASHUTILS}/bashutils.sh || { echo "Unable to source bashutils." ; exit 1 ; }
export BASHUTILS

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

START_TIME=$SECONDS

$(opt_parse \
    ":filter  f=${FILTER:-}    | Tests whose name or file amtch this will be run." \
    ":exclude x=${EXCLUDE:-}   | Tests whose name or file match this will not be run." \
    ":repeat  r=${REPEAT:-1}   | Number of times to repeat each test." \
    "+verbose v=${VERBOSE:-0}  | Verbose output." \
    ":debug   D=${EDEBUG:-}    | EDEBUG output." \
    "+break   b=${BREAK:-0}    | Stop immediately on first failure." \
    "+delete  d=1              | Delete all output files when tests complete." \
    "+clean   c=0              | Clean only and then exit." \
    "+html    h=0              | Produce an HTML logfile and strip color codes out of etest.log." \
    "TEST_DIR=$(readlink -f .) | Look for tests inside this directory.")

ETEST_TOPDIR=${TEST_DIR}
EDEBUG=${debug}

(( ${repeat} < 1 )) && repeat=1
[[ ${EDEBUG:-0} != "0" ]] && verbose=1 || true
edebug "$(lval TEST_DIR) $(opt_dump)"

if ! cgroup_supported ; then
    ewarn "Cgroups not supported on this system.  Process leaks will not be detected."
    export ETEST_CGROUP_BASE=unsupported
else
    # Global cgroup name for all unit tests
    export ETEST_CGROUP_BASE="etest/$(basename ${TEST_DIR})"
fi
export ETEST_CGROUP="${ETEST_CGROUP_BASE}/$$"

# Setup logfile
exec {ETEST_STDERR_FD}<&2
ETEST_LOG=${TEST_DIR}/etest.log
elogfile --rotate_count=10 --tail=${verbose} ${ETEST_LOG}

# Setup redirection for "etest" and actual "test" output
if [[ ${verbose} -eq 0 ]]; then
    ETEST_OUT="$(fd_path)/${ETEST_STDERR_FD}"
    TEST_OUT="/dev/null"
else
    ETEST_OUT="/dev/null"
    TEST_OUT="/dev/stderr"
fi

#-------------------------------------------------------------------------------------------------------------------------
# TEST UTILITY FUNCTIONS
#-------------------------------------------------------------------------------------------------------------------------

die_handler()
{
    $(opt_parse \
        ":rc return_code r=1 | Return code that die will exit with")

    # Append any error message to logfile
    if [[ ${verbose} -eq 0 ]]; then
        echo "" >&2
        eerror "${@}"
        
        # Call eerror_stacktrace but skip top three frames to skip over the frames
        # containing stacktrace_array, eerror_stacktrace and die itself. Also skip
        # over the initial error message since we already displayed it.
        eerror_stacktrace -f=4 -s

    fi &>${ETEST_OUT}
    
    exit ${rc}
}

etestmsg()
{
    EMSG_COLOR="all" emsg "magenta" "##" "WARN" "$@"
}

# Returns success if there are no stale processes remaining in the cgroup for
# this test and failure if there are any.
#
no_process_leaks_remain()
{
    $(tryrc -r=exists_rc cgroup_exists ${ETEST_CGROUP})

    # If the cgroup no longer exists, we're in good shape because you can't
    # destroy a cgroup until all its processes are dead.
    if [[ ${exists_rc} -ne 0 ]] ; then
        return 0
    fi

    # As long as it existed just now, we can assume cgroup_pids will exist,
    # because nothing else will destroy the cgroup except for us.
    local remaining_pids=$(cgroup_pids ${ETEST_CGROUP})
    edebug "$(lval remaining_pids exists_rc ETEST_CGROUP)"
    [[ -z ${remaining_pids} ]]
}

assert_no_process_leaks()
{
    # Error stacks generated here should produce output, even though etest has
    # them wrapped in a try.
    __BU_INSIDE_TRY=0

    edebug "Waiting..."
    
    # Wait for up to 5 seconds for leaked processes to die off.  If anything
    # lasts beyond that, we'll call it a test failure.
    $(tryrc eretry -T=5s -w=1 no_process_leaks_remain)

    # The above command could have timed out but that doesn't necessarily mean
    # there are leaked processes. So KILL anything that's left, but only DIE
    # if there were actually processes leaked.
    if [[ ${rc} -ne 0 ]] ; then
        local leaked_processes=$(cgroup_ps ${ETEST_CGROUP})
        if [[ -n ${leaked_processes} ]]; then
            cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP}

            die "Leaked processes in ${ETEST_CGROUP}:\n${leaked_processes}"
        fi
    fi

    edebug "Finished"
}

assert_no_mount_leaks()
{
    $(opt_parse path)
    edebug "Checking for stale mounts under $(lval path)"
 
    local mounts=( $(efindmnt "${path}" ) )
    
    if ! array_empty mounts; then
        eunmount -a -r -d=${delete} "${path}"
        die "Leaked mounts under $(lval path)\n$(array_join_nl mounts)"
    fi

    if [[ ${delete} -eq 1 ]]; then
        rm --recursive --force --verbose "${path}" |& edebug
    fi

    edebug "Finished"
} 

global_setup()
{
    edebug "Running global_setup"

    # Create a specific directory to run this test in. That way the test can create whatever directories and files it
    # needs and assuming the test succeeds we'll auto remove the directory after the test completes.
    TEST_DIR_OUTPUT="${TEST_DIR}/output"
    efreshdir ${TEST_DIR_OUTPUT}

    # Create a specific TMPDIR so that all calls to mktemp are under the test specific directory
    export TMPDIR=${TEST_DIR_OUTPUT}/tmp
    mkdir ${TMPDIR}
 
    if cgroup_supported ; then
        # And a cgroup that will contain all output
        cgroup_create ${ETEST_CGROUP}
        cgroup_move ${ETEST_CGROUP_BASE} $$
    fi

    edebug "Finished global_setup"
    return 0
}

global_teardown()
{
    [[ ${delete} -eq 0 ]] && edebug "Skipping global_teardown" && return 0
    edebug "Running global_teardown: PID=$$ BASHPID=${BASHPID} PPID=${PPID}"

    if cgroup_supported ; then
        cgroup_destroy -r ${ETEST_CGROUP}
    fi

    assert_no_mount_leaks ${TEST_DIR_OUTPUT}

    # Convert logfile to HTML if requested
    if [[ ${html} -eq 1 ]] && which ansi2html &>/dev/null; then
        edebug "Converting ${ETEST_LOG} into HTML"
        cat ${ETEST_LOG} | ansi2html --scheme=xterm > ${ETEST_LOG/.log/.html}
        sed -i "s:\x1B\[[0-9;]*[mK]::g" ${ETEST_LOG}
    fi

    edebug "Finished global_teardown"
    return 0
}

run_single_test()
{
    local testfile=$1
    local testfunc=$2
    local testfilename=$(basename ${testfile})
    local rc=0

    source ${testfile}
    declare -f ${testfunc} &>/dev/null || return 0

    ebanner "${testfunc}" REPEAT=REPEAT_STRING
 
    einfos ${testfunc} &>${ETEST_OUT}
   
    # We want to make sure that any traps from the tests
    # execute _before_ we run teardown, and also we don't
    # want the teardown to run inside the test-specific
    # cgroup.  This subshell solves both issues.
    try
    {
        # Pretend that the test _not_ executing inside a try/catch so that the
        # error stack will get printed if part of the test fails, as if etest
        # weren't running it inside a try/catch
        __BU_INSIDE_TRY=0

        # Create a specific TMPDIR so that all calls to mktemp are under the test specific directory
        export TMPDIR=${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}/tmp
        efreshdir ${TMPDIR}
 
        if cgroup_supported ; then
            cgroup_create ${ETEST_CGROUP}
            cgroup_move ${ETEST_CGROUP} ${BASHPID}
        fi

        # Unit test provided setup
        if declare -f setup &>/dev/null ; then
            etestmsg "Calling test_setup"
            setup
        fi
        
        cd ${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}

        etestmsg "Calling test"
        ${testfunc}
    }
    catch
    {
        rc=$?
    }

    local process_leak_rc=0
    if cgroup_supported ; then
        $(tryrc -r=process_leak_rc assert_no_process_leaks)
    fi

    local mount_leak_rc=0
    $(tryrc -r=mount_leak_rc assert_no_mount_leaks "${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}")

    if [[ ${rc} -eq 0 && ${process_leak_rc} -eq 0 && ${mount_leak_rc} -eq 0 ]]; then
        einfo "$(ecolor green)${testfunc} PASSED."
    elif [[ ${rc} -eq 0 && ${process_leak_rc} -ne 0 ]] ; then
        eerror "${testfunc} FAILED due to process leak."
        rc=1
    elif [[ ${rc} -eq 0 && ${mount_leak_rc} -ne 0 ]] ; then
        eerror "${testfunc} FAILED due to mount leak."
        rc=1
    else
        eerror "${testfunc} FAILED."
    fi

    eend ${rc} &>${ETEST_OUT}
    
    # Unit test provided teardown
    if declare -f teardown &>/dev/null ; then
        etestmsg "Calling test_teardown"
        $(tryrc -r=teardown_rc teardown)
    fi

    return ${rc}
}

run_all_tests_in_file()
{
    local testfile=$1
    local testfilename=$(basename ${testfile})

    # Skip files that don't contain any tests
    grep -q "^ETEST_" ${testfile} >/dev/null || return 0

    # Skip files whose name matches the exclude filter
    [[ -n ${exclude} && ${testfile} =~ ${exclude} ]] && return 0

    # Get all function names that begin with ETEST_ (and optionally match $2)
    if [[ -z ${filter} || ${testfile} =~ ${filter} ]]; then
        ETEST_FUNCTIONS=( $(source ${testfile}; declare -F | awk '$3 ~ "^ETEST" {print $3}' ) )
    else
        ETEST_FUNCTIONS=( $(source ${testfile}; declare -F | awk '$3 ~ "^ETEST" && $3 ~ "'${filter}'" {print $3}') )
    fi
    edebug $(lval testfile ETEST_FUNCTIONS)

    [[ ${#ETEST_FUNCTIONS[@]} -gt 0 ]] || return 0

    # Exclude those functions that match the exclude filter
    if [[ -n ${exclude} ]] ; then
        for index in "${!ETEST_FUNCTIONS[@]}" ; do
            if [[ ${ETEST_FUNCTIONS[$index]} =~ ${exclude} ]] ; then
                unset ETEST_FUNCTIONS[$index]
            fi
        done
    fi

    einfo "Running tests in ${testfile} ${REPEAT_STRING}" &>${ETEST_OUT}
    local file_start=${SECONDS}

    for testfunc in file_setup ${ETEST_FUNCTIONS[@]} file_teardown; do
     
        # Unit test infrastructure setup
        efreshdir ${TEST_DIR_OUTPUT}/${testfilename}/${testfunc}
     
        local test_rc=0
        try
        {
            # Pretend that the test _not_ executing inside a try/catch so that
            # the error stack will get printed if part of the test fails, as if
            # etest weren't running it inside a try/catch
            __EFUNCS_INSIDE_TRY=0

            ( run_single_test ${testfile} ${testfunc} )
        }
        catch
        {
            test_rc=$?
            [[ ${break} -eq 0 ]] || die "${testfunc} failed and break=1" &>${ETEST_OUT}
            FAILURES[${testfilename}]+="${testfunc} "
            [[ ${testfunc} != file_setup && ${testfunc} != file_teardown ]] && (( TEST_FAILED_COUNT += 1 ))
        }
        [[ ${testfunc} != file_setup && ${testfunc} != file_teardown ]] && (( TEST_EXECUTED_COUNT += 1 ))

    done

    einfos "Finished $(basename ${testfile}) in $(( SECONDS - file_start )) seconds.  ${REPEAT_STRING}" &>${ETEST_OUT}
}

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

declare -A FAILURES
global_setup
trap_add global_teardown

# If clean only is requested exit immediately. The "clean" is done via global_setup and global_teardown.
[[ ${clean} -eq 1 ]] && exit 0

#-------------------------------------------------------------------------------------------------------------------------
# MAIN
#-------------------------------------------------------------------------------------------------------------------------

TEST_EXECUTED_COUNT=0
TEST_FAILED_COUNT=0

for (( ITERATION=1; ITERATION<=${repeat}; ITERATION++ )); do
    [[ ${repeat} -gt 1 ]] && REPEAT_STRING="(${ITERATION}/${repeat})" || REPEAT_STRING=""

    # Run all standalone *.etest scripts directly outside of etest framework
    einfo "Running standalone tests ${REPEAT_STRING}" &>${ETEST_OUT}
    for filename in $(find ${TEST_DIR} -type f -name "*.etest" -executable | sort || true); do

        base=$(basename "${filename}" .etest)

        if [[ ( -z ${filter} || ${filter} =~ ${base} ) && ! ${exclude} =~ ${base} ]] ; then
            MSG="Running standalone $(lval script=filename)"
            einfos "ETEST_${base%%.etest}" &>${ETEST_OUT}
            ebanner "${MSG}"
            ${filename}
            eend &>${ETEST_OUT}
        fi

    done

    # Run *.etest files which are not executable and need to be sourced and run inside etest
    for filename in $(find ${TEST_DIR} -type f -name "*.etest" ! -executable | sort || true); do
        run_all_tests_in_file ${filename}
    done
done

changeset_info=""
if [[ -d ".hg" ]] ; then
    changeset_info=" $(hg id --id)"
elif [[ -d ".git" ]] ; then
    changeset_info=" $(git rev-parse --short HEAD)"
fi

{
    echo
    message="Finished testing $(basename ${ETEST_TOPDIR})${changeset_info}."
    message+=" $(( TEST_EXECUTED_COUNT - TEST_FAILED_COUNT))/${TEST_EXECUTED_COUNT} tests passed"
    message+=" in $(( SECONDS - START_TIME )) seconds."

    if [[ ${TEST_FAILED_COUNT} -gt 0 ]] ; then
        eerror "${message}"
    else
        einfo "${message}"
    fi
    echo

    if array_not_empty FAILURES; then
        eerror "FAILED TESTS:"
        for index in ${!FAILURES[@]} ; do
            for failed_test in ${FAILURES[$index]} ; do
                echo "$(ecolor "red")      ${failed_test}"
            done
        done
    fi
} |& tee -a ${ETEST_LOG} >&${ETEST_STDERR_FD}

exit ${TEST_FAILED_COUNT}
