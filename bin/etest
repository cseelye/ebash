#!/usr/bin/env bash

: ${EMSG_PREFIX:=time}

: ${BASHUTILS_HOME:=$(dirname $0)/..}
: ${BASHUTILS:=${BASHUTILS_HOME}/share}
source ${BASHUTILS}/bashutils.sh || { echo "Unable to source bashutils." ; exit 1 ; }
export BASHUTILS

if [[ ${__BU_OS} == "Linux" ]] ; then
    reexec --sudo
fi

# Normalize BASHUTILS path in case any tests depend on it looking... normal.
# Note: wait until after sourcing so that we can let bashutils make sure we get
# GNU readlink rater than BSD readlink.
BASHUTILS_HOME=$(readlink -f "${BASHUTILS_HOME}")
BASHUTILS=$(readlink -f "${BASHUTILS}")

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

$(opt_parse \
    ":filter  f=${FILTER:-}    | Tests whose name or file match this will be run." \
    ":exclude x=${EXCLUDE:-}   | Tests whose name or file match this will not be run." \
    ":repeat  r=${REPEAT:-1}   | Number of times to repeat each test." \
    "+verbose v=${VERBOSE:-0}  | Verbose output." \
    ":debug   D=${EDEBUG:-}    | EDEBUG output." \
    "+break   b=${BREAK:-0}    | Stop immediately on first failure." \
    "+delete  d=1              | Delete all output files when tests complete." \
    "+clean   c=0              | Clean only and then exit." \
    "+html    h=0              | Produce an HTML logfile and strip color codes out of etest.log." \
    "+mount_ns=1               | Run tests inside a mount namespace." \
    ":log_dir                  | Directory to place logs in.  Defaults to the current directory." \
    ":work_dir                 | Temporary location where etest can place temporary files.  This location will be both
                                 created and deleted by etest." \
    "&test_list l              | File that contains a list of tests to run.  This file may contain comments on lines
                                 that begin with the # character.  All other nonblank lines will be interpreted as
                                 things that could be passed as @tests -- directories, executable scripts, or .etest
                                 files.  Relative paths will be interpreted against the current directory.  This option
                                 may be specified multiple times." \
    "@tests                    | Any number of individual tests, which may be executables to be executed and checked for
                                 exit code or may be files whose names end in .etest, in which case they will be sourced
                                 and any test functions found will be executed.  You may also specify directories in
                                 which case etest will recursively find executables and .etest files and treat them in
                                 similar fashion.")

if [[ ${mount_ns} -eq 1 ]] && os linux; then
    reexec --mount-ns
fi

declare -A _BU_CONF
if [[ -r .bashutils ]] ; then
    conf_read _BU_CONF .bashutils
fi

START_TIME=$SECONDS

# Default log directory from conf file if unspecified on the command line
: ${log_dir:=$(conf_get _BU_CONF etest.log_dir)}
: ${log_dir:=.}
log_dir=$(readlink -f ${log_dir})

# Default set of tests from conf file if unspecified on the command line
if array_empty tests && array_empty test_list ; then
    tests=( $(conf_get _BU_CONF etest.tests) )
fi

if array_not_empty test_list ; then
    # Read non-comment lines from test_list and treat them as if they were passed as arguments to this script
    edebug "Grabbing test list from $(lval test_list)"
    array_init_nl tests_from_list "$(grep -vP '^\s*(#.*)$' "${test_list[@]}")"
    if array_not_empty tests_from_list ; then
        edebug "Found $(lval test_list tests_from_list)"
        tests+=( "${tests_from_list[@]}" )
    else
        edebug "Found no tests in $(lval test_list)"
    fi
fi

# Default working directory from conf file if unspecified, or ./output if not in either place
: ${work_dir:=$(conf_get _BU_CONF etest.work_dir)}
: ${work_dir:=./output}
work_dir=$(readlink -f ${work_dir})

EDEBUG=${debug}

(( ${repeat} < 1 )) && repeat=1
[[ ${EDEBUG:-0} != "0" ]] && verbose=1 || true
edebug "$(lval TEST_DIR) $(opt_dump)"

if ! cgroup_supported ; then
    export ETEST_CGROUP_BASE=unsupported
else
    # Global cgroup name for all unit tests run here
    export ETEST_CGROUP_BASE="etest"
fi
export ETEST_CGROUP="${ETEST_CGROUP_BASE}/$$"

# Setup logfile
exec {ETEST_STDERR_FD}<&2
ETEST_LOG=${log_dir}/etest.log
elogfile --rotate_count=10 --tail=${verbose} ${ETEST_LOG}

# Setup redirection for "etest" and actual "test" output
if [[ ${verbose} -eq 0 ]]; then
    ETEST_OUT="$(fd_path)/${ETEST_STDERR_FD}"
    TEST_OUT="/dev/null"
else
    ETEST_OUT="/dev/null"
    TEST_OUT="/dev/stderr"
fi

#-------------------------------------------------------------------------------------------------------------------------
# TEST UTILITY FUNCTIONS
#-------------------------------------------------------------------------------------------------------------------------

die_handler()
{
    $(opt_parse \
        ":rc return_code r=1 | Return code that die will exit with")

    # Append any error message to logfile
    if [[ ${verbose} -eq 0 ]]; then
        echo "" >&2
        eerror "${@}"
        
        # Call eerror_stacktrace but skip top three frames to skip over the frames
        # containing stacktrace_array, eerror_stacktrace and die itself. Also skip
        # over the initial error message since we already displayed it.
        eerror_stacktrace -f=4 -s

    fi &>${ETEST_OUT}
    
    exit ${rc}
}

# Returns success if there are no stale processes remaining in the cgroup for
# this test and failure if there are any.
#
no_process_leaks_remain()
{
    if cgroup_supported ; then
        $(tryrc -r=exists_rc cgroup_exists ${ETEST_CGROUP})

        # If the cgroup no longer exists, we're in good shape because you can't
        # destroy a cgroup until all its processes are dead.
        if [[ ${exists_rc} -ne 0 ]] ; then
            return 0
        fi

        # As long as it existed just now, we can assume cgroup_pids will exist,
        # because nothing else will destroy the cgroup except for us.
        local remaining_pids=$(cgroup_pids ${ETEST_CGROUP})
        edebug "$(lval remaining_pids exists_rc ETEST_CGROUP)"
        [[ -z ${remaining_pids} ]]
    fi
}

assert_no_process_leaks()
{
    # Error stacks generated here should produce output, even though etest has
    # them wrapped in a try.
    __BU_INSIDE_TRY=0

    edebug "Waiting..."
    
    # Wait for up to 5 seconds for leaked processes to die off.  If anything
    # lasts beyond that, we'll call it a test failure.
    $(tryrc eretry -T=5s no_process_leaks_remain)

    # The above command could have timed out but that doesn't necessarily mean
    # there are leaked processes. So KILL anything that's left, but only DIE
    # if there were actually processes leaked.
    if [[ ${rc} -ne 0 ]] && cgroup_supported ; then
        local leaked_processes=$(cgroup_ps ${ETEST_CGROUP})
        if [[ -n ${leaked_processes} ]]; then
            cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP}

            die "Leaked processes in ${ETEST_CGROUP}:\n${leaked_processes}"
        fi
    fi

    edebug "Finished"
}

assert_no_mount_leaks()
{
    $(opt_parse path)
    edebug "Checking for stale mounts under $(lval path)"
 
    local mounts=( $(efindmnt "${path}" ) )
    
    if ! array_empty mounts; then
        eunmount -a -r -d=${delete} "${path}"
        eerror "Leaked under $(lval mounts path)"$'\n'"$(array_join_nl mounts)"
        die "Leaked mounts"
    fi

    if [[ ${delete} -eq 1 ]]; then
        rm --recursive --force "${path}"
    fi

    edebug "Finished"
} 

global_setup()
{
    edebug "Running global_setup"

    # Create a specific directory to run this test in. That way the test can create whatever directories and files it
    # needs and assuming the test succeeds we'll auto remove the directory after the test completes.
    efreshdir "${work_dir}"

    if cgroup_supported ; then
        # And a cgroup that will contain all output
        cgroup_create ${ETEST_CGROUP}
        cgroup_move ${ETEST_CGROUP_BASE} $$
    fi

    edebug "Finished global_setup"
    return 0
}

global_teardown()
{
    [[ ${delete} -eq 0 ]] && edebug "Skipping global_teardown" && return 0
    edebug "Running global_teardown: PID=$$ BASHPID=${BASHPID} PPID=${PPID}"

    assert_no_process_leaks
    assert_no_mount_leaks ${work_dir}

    if cgroup_supported ; then
        cgroup_destroy -r ${ETEST_CGROUP}
    fi

    # Convert logfile to HTML if requested
    if [[ ${html} -eq 1 ]] && which ansi2html &>/dev/null; then
        edebug "Converting ${ETEST_LOG} into HTML"
        cat ${ETEST_LOG} | ansi2html --scheme=xterm > ${ETEST_LOG/.log/.html}
        noansi ${ETEST_LOG}
    fi

    edebug "Finished global_teardown"
    return 0
}

run_single_test()
{
    $(opt_parse \
        ":work_dir | Temporary directory that the test should use as its current working directory." \
        ":source   | Name file to be sourced in the shell that will run the test.  Most useful if the test is a function
                     inside that file." \
        "testname  | Command to execute to run the test")

    local rc=0

    # If the test name or optional source name doesn't match a specified FILTER, we're done
    if ! [[ -z ${filter} || ${testname} =~ ${filter} || ( -n ${source} && ${source} =~ ${filter} ) ]] ; then
        return 0
    fi

    # If the test name does match a specified EXCLUDE, we're done
    if [[ -n ${exclude} && ${testname} =~ ${exclude} ]] ; then
        return 0
    fi

    local display_testname=${testname}
    if [[ -n ${source} ]] ; then
        display_testname="${source}:${testname}"
    fi

    ebanner "${display_testname}" REPEAT=REPEAT_STRING
 
    einfos ${display_testname} &>${ETEST_OUT}

    (( TEST_EXECUTED_COUNT += 1 ))
   
    # We want to make sure that any traps from the tests
    # execute _before_ we run teardown, and also we don't
    # want the teardown to run inside the test-specific
    # cgroup.  This subshell solves both issues.
    try
    {
        export BASHUTILS BASHUTILS_HOME TEST_DIR_OUTPUT=${work_dir}
        if [[ -n ${source} ]] ; then
            source "${source}"
        fi

        # Pretend that the test _not_ executing inside a try/catch so that the
        # error stack will get printed if part of the test fails, as if etest
        # weren't running it inside a try/catch
        __BU_INSIDE_TRY=0

        # Create our temporary workspace in the directory specified by the caller
        efreshdir "${work_dir}"
        mkdir "${work_dir}/tmp"
        export TMPDIR="$(readlink -m ${work_dir}/tmp)"
 
        if cgroup_supported ; then
            cgroup_create ${ETEST_CGROUP}
            cgroup_move ${ETEST_CGROUP} ${BASHPID}
        fi

        # Unit test provided setup
        if is_function setup ; then
            etestmsg "Calling setup"
            setup
        fi

        local command="${testname}"
        if ! is_function "${testname}" ; then
            command=$(readlink -f "${testname}")
        fi
        
        cd "${work_dir}"

        etestmsg "Calling test"
        "${command}"

        if is_function teardown ; then
            etestmsg "Calling teardown"
            teardown
        fi
    }
    catch
    {
        rc=$?
    }
    edebug "Finished $(lval testname display_testname rc)"

    local process_leak_rc=0
    if cgroup_supported ; then
        $(tryrc -r=process_leak_rc assert_no_process_leaks)
    fi

    local mount_leak_rc=0
    $(tryrc -r=mount_leak_rc assert_no_mount_leaks "${work_dir}")

    if [[ ${rc} -eq 0 && ${process_leak_rc} -eq 0 && ${mount_leak_rc} -eq 0 ]]; then
        einfo "$(ecolor green)${display_testname} PASSED."

    elif [[ ${rc} -eq 0 && ${process_leak_rc} -ne 0 ]] ; then
        eerror "${display_testname} FAILED due to process leak."
        (( TEST_FAILED_COUNT += 1 ))
        FAILURES+=( "${display_testname}" )
        rc=1

    elif [[ ${rc} -eq 0 && ${mount_leak_rc} -ne 0 ]] ; then
        eerror "${display_testname} FAILED due to mount leak."
        (( TEST_FAILED_COUNT += 1 ))
        FAILURES+=( "${display_testname}" )
        rc=1

    else
        eerror "${display_testname} FAILED."
        (( TEST_FAILED_COUNT += 1 ))
        FAILURES+=( "${display_testname}" )
    fi

    eend ${rc} &>${ETEST_OUT}
    
    # Unit test provided teardown
    if declare -f teardown &>/dev/null ; then
        etestmsg "Calling test_teardown"
        $(tryrc -r=teardown_rc teardown)
    fi

    # NOTE: Don't return rc here.  We've already set things up so etest knows if there was a failure.
}

run_etest_file()
{
    $(opt_parse testfile)
    local testfilename=$(basename ${testfile})

    # If the test file name does match a specified exclude, we're done
    if [[ -n ${exclude} && ${testfile} =~ ${exclude} ]] ; then
        return 0
    fi

    # Get all function names that begin with ETEST_
    edebug $(lval testfile ETEST_FUNCTIONS)
    ETEST_FUNCTIONS=( $(source "${testfile}" ; declare -F | awk '$3 ~ "^ETEST" {print $3}' || true) )

    if [[ ${#ETEST_FUNCTIONS[@]} -eq 0 ]] ; then
        ewarn "No tests found in $(lval testfile)"
        return 0
    fi

    local testfunc
    for testfunc in ${ETEST_FUNCTIONS[@]} ; do

        local test_work_dir="${work_dir}/${testfilename}/${testfunc}"
        run_single_test --work-dir "${test_work_dir}" --source "${testfile}" "${testfunc}"

        if [[ ${break} -eq 1 && TEST_FAILED_COUNT -gt 0 ]] ; then
            die "${testfunc} failed and break=1" &>${ETEST_OUT}
        fi

    done
}

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

declare -ag FAILURES
global_setup
trap_add global_teardown

# If clean only is requested exit immediately. The "clean" is done via global_setup and global_teardown.
[[ ${clean} -eq 1 ]] && exit 0

#-------------------------------------------------------------------------------------------------------------------------
# MAIN
#-------------------------------------------------------------------------------------------------------------------------

TEST_EXECUTED_COUNT=0
TEST_FAILED_COUNT=0


for (( ITERATION=1; ITERATION<=${repeat}; ITERATION++ )); do
    [[ ${repeat} -gt 1 ]] && REPEAT_STRING="(${ITERATION}/${repeat})" || REPEAT_STRING=""

    for test_location in "${tests[@]}" ; do

        # Run all standalone *.etest scripts directly outside of etest framework
        for filename in $(find "${test_location}" -type f -executable | sort || true); do
            run_single_test --work-dir "${work_dir}/standalone" "${filename}"
        done

        # Run *.etest files which are not executable and need to be sourced and run inside etest
        for filename in $(find "${test_location}" -type f -name "*.etest" | sort || true); do
            run_etest_file "${filename}"
        done

    done
done

changeset_info=""
if [[ -d ".hg" ]] ; then
    changeset_info=" $(hg id --id)"
elif [[ -d ".git" ]] ; then
    changeset_info=" $(git rev-parse --short HEAD)"
fi

{
    echo
    message="Finished testing${changeset_info}."
    message+=" $(( TEST_EXECUTED_COUNT - TEST_FAILED_COUNT))/${TEST_EXECUTED_COUNT} tests passed"
    message+=" in $(( SECONDS - START_TIME )) seconds."

    if [[ ${TEST_FAILED_COUNT} -gt 0 ]] ; then
        eerror "${message}"
    else
        einfo "${message}"
    fi
    echo

    if array_not_empty FAILURES; then
        eerror "FAILED TESTS:"
        for failed_test in "${FAILURES[@]}" ; do
            echo "$(ecolor "red")      ${failed_test}" >&2
        done
        ecolor off >&2
    fi
} |& tee -a ${ETEST_LOG} >&${ETEST_STDERR_FD}

exit ${TEST_FAILED_COUNT}
